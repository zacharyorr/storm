{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Storm: Linking Predicted SCS Wind Likelihood to State-Level Power Outages\n",
    "\n",
    "**Objective:** Analyze the relationship between the predicted monthly likelihood/frequency of Severe Convective Storms (SCS) wind events (aggregated to the state level) and observed state-level monthly power outage metrics.\n",
    "\n",
    "**Methodology:**\n",
    "1. Load and clean historical power outage data from `outages.xlsx`.\n",
    "2. Aggregate outage data to meaningful monthly metrics per state.\n",
    "3. Load the previously trained multi-state SCS prediction model and scaler.\n",
    "4. Generate historical monthly *predicted expected hits* per county using the model.\n",
    "5. Aggregate predicted hits to the **state-month** level.\n",
    "6. Merge state-level outage metrics and state-level predicted storm likelihood.\n",
    "7. Analyze correlations and potentially build a simple regression model (`State Outage ~ State Predicted Hits`).\n",
    "\n",
    "**Data Requirements:**\n",
    "* `outages.xlsx`: State-level monthly outage data. Sheet: `250409 Outage Data Analysis vSh`.\n",
    "* `county_monthly_climate_variables_target_states.parquet` (or `.csv`): Processed climate data for the 14 target states (needs to cover historical period 2015-2024).\n",
    "* `scs_wind_target_AnomInd_lgbm_calibrated.joblib`: Trained multi-state SCS occurrence prediction model.\n",
    "* `scs_wind_target_states_scaler.joblib`: Scaler fitted on training data for the SCS model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-14 13:23:48] [INFO] OutageAnalysisLogger - --- Outage Analysis Notebook Started (Event Type: scs_wind) ---\n",
      "[2025-04-14 13:23:48] [INFO] OutageAnalysisLogger - Processing target states: ['12', '20', '29', '40', '48', '05', '22', '28', '01', '13', '45', '37', '47', '51']\n",
      "[2025-04-14 13:23:48] [INFO] OutageAnalysisLogger - Using Outage File: ../data/outages/outages.xlsx\n",
      "[2025-04-14 13:23:48] [INFO] OutageAnalysisLogger - Max Evaluation Year set to: 2024\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Setup & Configuration ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # For plotting correlations\n",
    "from sklearn.preprocessing import StandardScaler # For loading scaler\n",
    "import lightgbm as lgb # For loading model object structure\n",
    "from sklearn.calibration import CalibratedClassifierCV # For loading model object structure\n",
    "import gc\n",
    "\n",
    "# --- Configuration ---\n",
    "# --- Input Paths ---\n",
    "CLIMATE_FILE = \"../output/processed_climate/county_monthly_climate_variables_target_states.parquet\"\n",
    "MODEL_PATH = \"../output/models/scs_wind_target_AnomInd_lgbm_calibrated.joblib\" # Path to calibrated binary classifier\n",
    "SCALER_PATH = \"../output/models/scs_wind_target_states_scaler.joblib\" # Path to scaler for Anom+Index features\n",
    "OUTAGE_FILE_PATH = \"../data/outages/outages.xlsx\"\n",
    "OUTAGE_SHEET_NAME = '250409 Outage Data Analysis vSh' # Or 0 for first sheet\n",
    "COUNTIES_DEF_FILE = \"../data/counties/2024_counties.txt\"\n",
    "\n",
    "# --- Parameters ---\n",
    "TARGET_EVENT_TYPE = 'scs_wind'\n",
    "target_state_fips = [ # List of 14 states\n",
    "    \"12\", \"20\", \"29\", \"40\", \"48\", \"05\", \"22\", \"28\",\n",
    "    \"01\", \"13\", \"45\", \"37\", \"47\", \"51\"\n",
    "]\n",
    "TARGET_OUTAGE_METRICS = [ # Columns available in outages.xlsx\n",
    "    'CustomerHoursOutTotal', 'MaxCustomersOutTotal', 'CustomersTrackedTotal'\n",
    "]\n",
    "# Map State Names (from outage file) to State FIPS\n",
    "STATE_NAME_TO_FIPS = { # Add ALL states present in outage file that are also in target_state_fips\n",
    "    'Florida': '12', 'Kansas': '20', 'Missouri': '29', 'Oklahoma': '40',\n",
    "    'Texas': '48', 'Arkansas': '05', 'Louisiana': '22', 'Mississippi': '28',\n",
    "    'Alabama': '01', 'Georgia': '13', 'South Carolina': '45',\n",
    "    'North Carolina': '37', 'Tennessee': '47', 'Virginia': '51'\n",
    "}\n",
    "\n",
    "# --- !! ADDED: Define the last year for historical matching !! ---\n",
    "MAX_EVALUATION_YEAR = 2024 # Use the last year of your reliable historical EVENT data period\n",
    "\n",
    "# --- Output Paths ---\n",
    "ANALYSIS_OUTPUT_DIR = \"../output/analysis\"\n",
    "os.makedirs(ANALYSIS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Setup Logger ---\n",
    "LOG_FILE = \"../logs/08_outage_analysis_log.log\"\n",
    "logger = logging.getLogger(\"OutageAnalysisLogger\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "if not logger.handlers:\n",
    "    fh = logging.FileHandler(LOG_FILE, mode='w'); fh.setLevel(logging.DEBUG); fh_formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"); fh.setFormatter(fh_formatter); logger.addHandler(fh)\n",
    "    sh = logging.StreamHandler(); sh.setLevel(logging.INFO); sh_formatter = logging.Formatter(\"[%(asctime)s] [%(levelname)s] %(name)s - %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"); sh.setFormatter(sh_formatter); logger.addHandler(sh)\n",
    "\n",
    "logger.info(f\"--- Outage Analysis Notebook Started (Event Type: {TARGET_EVENT_TYPE}) ---\")\n",
    "logger.info(f\"Processing target states: {target_state_fips}\")\n",
    "logger.info(f\"Using Outage File: {OUTAGE_FILE_PATH}\")\n",
    "logger.info(f\"Max Evaluation Year set to: {MAX_EVALUATION_YEAR}\") # Log the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-14 13:25:35] [INFO] OutageAnalysisLogger - --- Loading and Cleaning State-Level Outage Data from First Sheet ---\n",
      "[2025-04-14 13:25:35] [INFO] OutageAnalysisLogger - Loading outage data from first sheet of ../data/outages/outages.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-14 13:25:37] [INFO] OutageAnalysisLogger - Loaded 5916 outage records initially.\n",
      "[2025-04-14 13:25:37] [INFO] OutageAnalysisLogger - Mapping State names to State FIPS...\n",
      "[2025-04-14 13:25:37] [INFO] OutageAnalysisLogger - Mapped 4872 records.\n",
      "[2025-04-14 13:25:37] [WARNING] OutageAnalysisLogger - Could not map states: ['California' 'Arizona' 'New Mexico'].\n",
      "[2025-04-14 13:25:37] [INFO] OutageAnalysisLogger - Filtering outage data for target states FIPS: ['12', '20', '29', '40', '48', '05', '22', '28', '01', '13', '45', '37', '47', '51']...\n",
      "[2025-04-14 13:25:37] [INFO] OutageAnalysisLogger - Filtered from 5916 to 4872 records.\n",
      "[2025-04-14 13:25:37] [INFO] OutageAnalysisLogger - Creating monthly timestamp...\n",
      "[2025-04-14 13:25:37] [INFO] OutageAnalysisLogger - Converting metrics ['CustomerHoursOutTotal', 'MaxCustomersOutTotal', 'CustomersTrackedTotal'] to numeric...\n",
      "[2025-04-14 13:25:37] [WARNING] OutageAnalysisLogger - NaN values found in metrics:\n",
      "CustomerHoursOutTotal    3580\n",
      "MaxCustomersOutTotal     3580\n",
      "CustomersTrackedTotal    3580\n",
      "dtype: int64\n",
      "[2025-04-14 13:25:37] [INFO] OutageAnalysisLogger - Dropping rows with NaN in key numeric/ID columns...\n",
      "[2025-04-14 13:25:37] [WARNING] OutageAnalysisLogger - Dropped 3580 rows with NaNs in key metrics/IDs.\n",
      "[2025-04-14 13:25:37] [INFO] OutageAnalysisLogger - Calculating Peak Pct Out...\n",
      "C:\\Users\\60864\\AppData\\Local\\Temp\\ipykernel_36664\\2302640883.py:74: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_outages_state['peak_pct_out'].fillna(0, inplace=True)\n",
      "[2025-04-14 13:25:37] [INFO] OutageAnalysisLogger - State-level outage data loading and cleaning complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cleaned State-Level Monthly Outage Data Sample ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_fips</th>\n",
       "      <th>time</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>CustomerHoursOutTotal</th>\n",
       "      <th>MaxCustomersOutTotal</th>\n",
       "      <th>CustomersTrackedTotal</th>\n",
       "      <th>peak_pct_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>01</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>1.872511e+04</td>\n",
       "      <td>6418.0</td>\n",
       "      <td>9509.0</td>\n",
       "      <td>67.493953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>01</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>4.108071e+04</td>\n",
       "      <td>11289.0</td>\n",
       "      <td>18911.0</td>\n",
       "      <td>59.695415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>01</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2017</td>\n",
       "      <td>8</td>\n",
       "      <td>2.891273e+04</td>\n",
       "      <td>13160.0</td>\n",
       "      <td>19068.0</td>\n",
       "      <td>69.016153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>01</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>6.810718e+05</td>\n",
       "      <td>26006.0</td>\n",
       "      <td>93347.0</td>\n",
       "      <td>27.859492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>01</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>1.339345e+06</td>\n",
       "      <td>100907.0</td>\n",
       "      <td>818476.0</td>\n",
       "      <td>12.328645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state_fips       time  Year  Month  CustomerHoursOutTotal  \\\n",
       "3737         01 2017-06-01  2017      6           1.872511e+04   \n",
       "3738         01 2017-07-01  2017      7           4.108071e+04   \n",
       "3739         01 2017-08-01  2017      8           2.891273e+04   \n",
       "3740         01 2017-09-01  2017      9           6.810718e+05   \n",
       "3741         01 2017-10-01  2017     10           1.339345e+06   \n",
       "\n",
       "      MaxCustomersOutTotal  CustomersTrackedTotal  peak_pct_out  \n",
       "3737                6418.0                 9509.0     67.493953  \n",
       "3738               11289.0                18911.0     59.695415  \n",
       "3739               13160.0                19068.0     69.016153  \n",
       "3740               26006.0                93347.0     27.859492  \n",
       "3741              100907.0               818476.0     12.328645  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cleaned Outage Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1292 entries, 3737 to 5219\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   state_fips             1292 non-null   object        \n",
      " 1   time                   1292 non-null   datetime64[ns]\n",
      " 2   Year                   1292 non-null   int64         \n",
      " 3   Month                  1292 non-null   int64         \n",
      " 4   CustomerHoursOutTotal  1292 non-null   float64       \n",
      " 5   MaxCustomersOutTotal   1292 non-null   float64       \n",
      " 6   CustomersTrackedTotal  1292 non-null   float64       \n",
      " 7   peak_pct_out           1292 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(4), int64(2), object(1)\n",
      "memory usage: 90.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Load and Clean Outage Data (State Level - Corrected Var Name) ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging # Ensure logger is available\n",
    "import gc # For garbage collection\n",
    "\n",
    "# Ensure target_state_fips and STATE_NAME_TO_FIPS exist from Cell #2\n",
    "if 'target_state_fips' not in locals() or not target_state_fips: raise NameError(\"target_state_fips list not defined.\")\n",
    "if 'STATE_NAME_TO_FIPS' not in locals(): raise NameError(\"STATE_NAME_TO_FIPS mapping not defined.\")\n",
    "if 'OUTAGE_FILE_PATH' not in locals(): raise NameError(\"OUTAGE_FILE_PATH not defined.\")\n",
    "# *** Check for the CORRECT variable name from Cell #2 ***\n",
    "if 'TARGET_OUTAGE_METRICS' not in locals() or not TARGET_OUTAGE_METRICS:\n",
    "     raise NameError(\"TARGET_OUTAGE_METRICS list not defined or empty in Cell #2.\")\n",
    "\n",
    "df_outages = pd.DataFrame() # Initialize\n",
    "logger.info(\"--- Loading and Cleaning State-Level Outage Data from First Sheet ---\")\n",
    "try:\n",
    "    # --- Load Data ---\n",
    "    if not os.path.exists(OUTAGE_FILE_PATH): raise FileNotFoundError(f\"Outage file not found: {OUTAGE_FILE_PATH}\")\n",
    "\n",
    "    logger.info(f\"Loading outage data from first sheet of {OUTAGE_FILE_PATH}...\")\n",
    "    df_outages_raw = pd.read_excel(OUTAGE_FILE_PATH, sheet_name=0, dtype={'CountyFIPS': str}) # Read first sheet\n",
    "    logger.info(f\"Loaded {len(df_outages_raw)} outage records initially.\")\n",
    "    logger.debug(f\"Initial columns: {df_outages_raw.columns.tolist()}\")\n",
    "\n",
    "    # --- Initial Cleaning & Validation ---\n",
    "    # *** Use CORRECT variable TARGET_OUTAGE_METRICS ***\n",
    "    required_cols = ['STATE', 'Year', 'Month'] + TARGET_OUTAGE_METRICS\n",
    "    missing_cols = [col for col in required_cols if col not in df_outages_raw.columns]\n",
    "    if missing_cols: raise ValueError(f\"Outage data missing required columns: {missing_cols}.\")\n",
    "\n",
    "    # 1. Map State Name to State FIPS\n",
    "    logger.info(\"Mapping State names to State FIPS...\")\n",
    "    df_outages_raw['state_fips'] = df_outages_raw['STATE'].map(STATE_NAME_TO_FIPS)\n",
    "    # ... (rest of mapping logic) ...\n",
    "    mapped_count = df_outages_raw['state_fips'].notna().sum(); unmapped_states = df_outages_raw[df_outages_raw['state_fips'].isna()]['STATE'].unique(); logger.info(f\"Mapped {mapped_count} records.\");\n",
    "    if len(unmapped_states) > 0: logger.warning(f\"Could not map states: {unmapped_states}.\")\n",
    "\n",
    "    # 2. Filter for Target States using FIPS\n",
    "    logger.info(f\"Filtering outage data for target states FIPS: {target_state_fips}...\")\n",
    "    initial_rows = len(df_outages_raw)\n",
    "    df_outages_state = df_outages_raw[df_outages_raw['state_fips'].isin(target_state_fips)].copy()\n",
    "    logger.info(f\"Filtered from {initial_rows} to {len(df_outages_state)} records.\")\n",
    "    if df_outages_state.empty: logger.warning(\"Outage DataFrame empty after state filter!\")\n",
    "\n",
    "    # 3. Create Monthly Timestamp\n",
    "    logger.info(\"Creating monthly timestamp...\")\n",
    "    df_outages_state['Year'] = pd.to_numeric(df_outages_state['Year'], errors='coerce'); df_outages_state['Month'] = pd.to_numeric(df_outages_state['Month'], errors='coerce')\n",
    "    df_outages_state.dropna(subset=['Year', 'Month'], inplace=True); df_outages_state['Year'] = df_outages_state['Year'].astype(int); df_outages_state['Month'] = df_outages_state['Month'].astype(int)\n",
    "    df_outages_state['time'] = pd.to_datetime(df_outages_state[['Year', 'Month']].assign(DAY=1))\n",
    "\n",
    "    # 4. Convert Metrics to Numeric (Use CORRECT variable TARGET_OUTAGE_METRICS)\n",
    "    logger.info(f\"Converting metrics {TARGET_OUTAGE_METRICS} to numeric...\")\n",
    "    for col in TARGET_OUTAGE_METRICS:\n",
    "        df_outages_state[col] = pd.to_numeric(df_outages_state[col], errors='coerce')\n",
    "    nan_counts = df_outages_state[TARGET_OUTAGE_METRICS].isnull().sum()\n",
    "    if nan_counts.sum() > 0: logger.warning(f\"NaN values found in metrics:\\n{nan_counts[nan_counts > 0]}\")\n",
    "    # Drop rows where essential numbers are missing? Or impute? Let's drop for now.\n",
    "    logger.info(\"Dropping rows with NaN in key numeric/ID columns...\")\n",
    "    key_numeric_cols = ['state_fips', 'time', 'CustomersTrackedTotal', 'MaxCustomersOutTotal', 'CustomerHoursOutTotal'] # Adjust as needed\n",
    "    initial_rows_dropna = len(df_outages_state)\n",
    "    df_outages_state.dropna(subset=[col for col in key_numeric_cols if col in df_outages_state.columns], inplace=True)\n",
    "    if len(df_outages_state) < initial_rows_dropna : logger.warning(f\"Dropped {initial_rows_dropna - len(df_outages_state)} rows with NaNs in key metrics/IDs.\")\n",
    "\n",
    "\n",
    "    # 5. Calculate Percentage Metric\n",
    "    logger.info(\"Calculating Peak Pct Out...\")\n",
    "    tracked = df_outages_state['CustomersTrackedTotal']; max_out_col = 'MaxCustomersOutTotal'\n",
    "    if max_out_col not in df_outages_state.columns: logger.warning(f\"'{max_out_col}' not found.\"); df_outages_state['peak_pct_out'] = np.nan\n",
    "    else:\n",
    "        out = df_outages_state[max_out_col]\n",
    "        df_outages_state['peak_pct_out'] = np.where(tracked.notna() & (tracked > 0) & out.notna(), (out / tracked) * 100, np.nan)\n",
    "    df_outages_state['peak_pct_out'].fillna(0, inplace=True)\n",
    "\n",
    "    # 6. Select and Sort Final Columns\n",
    "    final_cols = ['state_fips', 'time', 'Year', 'Month'] + TARGET_OUTAGE_METRICS + ['peak_pct_out'] # Use TARGET_OUTAGE_METRICS\n",
    "    df_outages_state = df_outages_state[[col for col in final_cols if col in df_outages_state.columns]].copy()\n",
    "    df_outages_state.sort_values(by=['state_fips', 'time'], inplace=True)\n",
    "\n",
    "    if df_outages_state.empty: logger.warning(\"State outage DataFrame is empty after cleaning!\")\n",
    "    else:\n",
    "        logger.info(\"State-level outage data loading and cleaning complete.\")\n",
    "        print(\"\\n--- Cleaned State-Level Monthly Outage Data Sample ---\"); display(df_outages_state.head())\n",
    "        print(\"\\n--- Cleaned Outage Data Info ---\"); df_outages_state.info()\n",
    "\n",
    "except FileNotFoundError as fnf: logger.exception(f\"Outage file error: {fnf}\"); raise fnf\n",
    "# ... (rest of except blocks) ...\n",
    "except Exception as e: logger.exception(f\"Failed loading/cleaning outage data: {e}\"); raise e\n",
    "\n",
    "# Clean up\n",
    "if 'df_outages_raw' in locals(): del df_outages_raw; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-14 13:25:42] [INFO] OutageAnalysisLogger - --- Preparing Final Monthly Outage Metrics ---\n",
      "[2025-04-14 13:25:42] [INFO] OutageAnalysisLogger - Selecting final metric columns: ['state_fips', 'time', 'CustomerHoursOutTotal', 'MaxCustomersOutTotal', 'CustomersTrackedTotal', 'peak_pct_out']\n",
      "[2025-04-14 13:25:42] [INFO] OutageAnalysisLogger - Final monthly outage data prepared. Shape: (1292, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Monthly Outage Data Sample ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state_fips</th>\n",
       "      <th>time</th>\n",
       "      <th>CustomerHoursOutTotal</th>\n",
       "      <th>MaxCustomersOutTotal</th>\n",
       "      <th>CustomersTrackedTotal</th>\n",
       "      <th>peak_pct_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>01</td>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>1.872511e+04</td>\n",
       "      <td>6418.0</td>\n",
       "      <td>9509.0</td>\n",
       "      <td>67.493953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>01</td>\n",
       "      <td>2017-07-01</td>\n",
       "      <td>4.108071e+04</td>\n",
       "      <td>11289.0</td>\n",
       "      <td>18911.0</td>\n",
       "      <td>59.695415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>01</td>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>2.891273e+04</td>\n",
       "      <td>13160.0</td>\n",
       "      <td>19068.0</td>\n",
       "      <td>69.016153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>01</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>6.810718e+05</td>\n",
       "      <td>26006.0</td>\n",
       "      <td>93347.0</td>\n",
       "      <td>27.859492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>01</td>\n",
       "      <td>2017-10-01</td>\n",
       "      <td>1.339345e+06</td>\n",
       "      <td>100907.0</td>\n",
       "      <td>818476.0</td>\n",
       "      <td>12.328645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state_fips       time  CustomerHoursOutTotal  MaxCustomersOutTotal  \\\n",
       "3737         01 2017-06-01           1.872511e+04                6418.0   \n",
       "3738         01 2017-07-01           4.108071e+04               11289.0   \n",
       "3739         01 2017-08-01           2.891273e+04               13160.0   \n",
       "3740         01 2017-09-01           6.810718e+05               26006.0   \n",
       "3741         01 2017-10-01           1.339345e+06              100907.0   \n",
       "\n",
       "      CustomersTrackedTotal  peak_pct_out  \n",
       "3737                 9509.0     67.493953  \n",
       "3738                18911.0     59.695415  \n",
       "3739                19068.0     69.016153  \n",
       "3740                93347.0     27.859492  \n",
       "3741               818476.0     12.328645  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Cell 4: Prepare Final Monthly Outage Data (No Aggregation Needed) ---\n",
    "\n",
    "logger.info(\"--- Preparing Final Monthly Outage Metrics ---\")\n",
    "\n",
    "# df_outages_state should be available from Cell #3\n",
    "if 'df_outages_state' not in locals() or df_outages_state.empty:\n",
    "    logger.error(\"Cleaned outage data (df_outages_state) not available or empty. Skipping.\")\n",
    "    # Initialize df_outages_monthly_agg as empty to avoid NameError later\n",
    "    df_outages_monthly_agg = pd.DataFrame()\n",
    "else:\n",
    "    try:\n",
    "        # --- Select relevant columns based on TARGET_OUTAGE_METRICS from Cell #2 ---\n",
    "        cols_to_keep = ['state_fips', 'time'] + TARGET_OUTAGE_METRICS\n",
    "        # Also keep the calculated peak_pct_out if it exists\n",
    "        if 'peak_pct_out' in df_outages_state.columns:\n",
    "             cols_to_keep.append('peak_pct_out')\n",
    "        # Ensure all requested columns actually exist in df_outages_state\n",
    "        cols_present = [col for col in cols_to_keep if col in df_outages_state.columns]\n",
    "        missing_selection = list(set(cols_to_keep) - set(cols_present))\n",
    "        if missing_selection:\n",
    "             logger.warning(f\"Columns requested in cols_to_keep missing from df_outages_state: {missing_selection}\")\n",
    "\n",
    "        logger.info(f\"Selecting final metric columns: {cols_present}\")\n",
    "        df_outages_monthly_agg = df_outages_state[cols_present].copy()\n",
    "\n",
    "        # Optional: Rename columns for clarity if desired\n",
    "        # df_outages_monthly_agg.rename(columns={'CustomerHoursOutTotal': 'CustHrsOut', ...}, inplace=True)\n",
    "\n",
    "        if df_outages_monthly_agg.empty:\n",
    "            logger.warning(\"Monthly aggregated outage DataFrame is empty after column selection!\")\n",
    "        else:\n",
    "            logger.info(f\"Final monthly outage data prepared. Shape: {df_outages_monthly_agg.shape}\")\n",
    "            print(\"\\n--- Final Monthly Outage Data Sample ---\")\n",
    "            display(df_outages_monthly_agg.head())\n",
    "            # Check NaN counts in the final selected metrics\n",
    "            final_metric_cols = [col for col in cols_present if col not in ['state_fips', 'time']]\n",
    "            final_nan_counts = df_outages_monthly_agg[final_metric_cols].isnull().sum()\n",
    "            if final_nan_counts.sum() > 0:\n",
    "                 logger.warning(f\"NaN values present in final outage metrics:\\n{final_nan_counts[final_nan_counts > 0]}\")\n",
    "\n",
    "\n",
    "    except KeyError as ke:\n",
    "         logger.exception(f\"Column not found error during final preparation: {ke}\")\n",
    "         df_outages_monthly_agg = pd.DataFrame() # Ensure empty on error\n",
    "         raise ke\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed during final outage data preparation: {e}\")\n",
    "        df_outages_monthly_agg = pd.DataFrame() # Ensure empty on error\n",
    "        raise e\n",
    "\n",
    "# Clean up intermediate frame if it exists\n",
    "if 'df_outages_raw' in locals():\n",
    "    del df_outages_raw\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-14 13:25:45] [INFO] OutageAnalysisLogger - --- Loading Historical Climate Features for Target States ---\n",
      "[2025-04-14 13:25:45] [INFO] OutageAnalysisLogger - Loading historical climate features from: ../output/processed_climate/county_monthly_climate_variables_target_states.parquet\n",
      "[2025-04-14 13:25:45] [INFO] OutageAnalysisLogger - Loaded climate data shape: (362628, 10)\n",
      "[2025-04-14 13:25:45] [INFO] OutageAnalysisLogger - Filtering climate features for relevant historical period: 2015-2024\n",
      "[2025-04-14 13:25:45] [INFO] OutageAnalysisLogger - Historical climate features prepared. Shape: (172680, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_geoid</th>\n",
       "      <th>time</th>\n",
       "      <th>Max Temp</th>\n",
       "      <th>Spec Humid</th>\n",
       "      <th>Sens Heat Flux</th>\n",
       "      <th>Precip</th>\n",
       "      <th>Sea Level Press</th>\n",
       "      <th>Soil Moisture</th>\n",
       "      <th>Wind Speed</th>\n",
       "      <th>Altitude</th>\n",
       "      <th>year</th>\n",
       "      <th>state_fips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01001</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>23.411621</td>\n",
       "      <td>0.004888</td>\n",
       "      <td>28.097054</td>\n",
       "      <td>2.217419</td>\n",
       "      <td>1026.149536</td>\n",
       "      <td>31.223341</td>\n",
       "      <td>2.761991</td>\n",
       "      <td>143.030991</td>\n",
       "      <td>2015</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01001</td>\n",
       "      <td>2015-02-01</td>\n",
       "      <td>23.523560</td>\n",
       "      <td>0.006459</td>\n",
       "      <td>30.694408</td>\n",
       "      <td>4.660017</td>\n",
       "      <td>1023.142578</td>\n",
       "      <td>32.409649</td>\n",
       "      <td>3.117097</td>\n",
       "      <td>143.030991</td>\n",
       "      <td>2015</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01001</td>\n",
       "      <td>2015-03-01</td>\n",
       "      <td>28.190033</td>\n",
       "      <td>0.007170</td>\n",
       "      <td>58.857788</td>\n",
       "      <td>4.170286</td>\n",
       "      <td>1022.330811</td>\n",
       "      <td>31.807745</td>\n",
       "      <td>3.139431</td>\n",
       "      <td>143.030991</td>\n",
       "      <td>2015</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01001</td>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>31.445557</td>\n",
       "      <td>0.009836</td>\n",
       "      <td>61.469265</td>\n",
       "      <td>4.297930</td>\n",
       "      <td>1016.757202</td>\n",
       "      <td>30.847223</td>\n",
       "      <td>3.183161</td>\n",
       "      <td>143.030991</td>\n",
       "      <td>2015</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01001</td>\n",
       "      <td>2015-05-01</td>\n",
       "      <td>34.129486</td>\n",
       "      <td>0.013535</td>\n",
       "      <td>49.009747</td>\n",
       "      <td>5.193621</td>\n",
       "      <td>1018.477417</td>\n",
       "      <td>30.793398</td>\n",
       "      <td>2.228818</td>\n",
       "      <td>143.030991</td>\n",
       "      <td>2015</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  county_geoid       time   Max Temp  Spec Humid  Sens Heat Flux    Precip  \\\n",
       "0        01001 2015-01-01  23.411621    0.004888       28.097054  2.217419   \n",
       "1        01001 2015-02-01  23.523560    0.006459       30.694408  4.660017   \n",
       "2        01001 2015-03-01  28.190033    0.007170       58.857788  4.170286   \n",
       "3        01001 2015-04-01  31.445557    0.009836       61.469265  4.297930   \n",
       "4        01001 2015-05-01  34.129486    0.013535       49.009747  5.193621   \n",
       "\n",
       "   Sea Level Press  Soil Moisture  Wind Speed    Altitude  year state_fips  \n",
       "0      1026.149536      31.223341    2.761991  143.030991  2015         01  \n",
       "1      1023.142578      32.409649    3.117097  143.030991  2015         01  \n",
       "2      1022.330811      31.807745    3.139431  143.030991  2015         01  \n",
       "3      1016.757202      30.847223    3.183161  143.030991  2015         01  \n",
       "4      1018.477417      30.793398    2.228818  143.030991  2015         01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Cell 5: Load Historical Climate Features ---\n",
    "\n",
    "logger.info(\"--- Loading Historical Climate Features for Target States ---\")\n",
    "\n",
    "df_climate_hist = pd.DataFrame() # Initialize\n",
    "# feature_cols should be defined from the modeling notebook context\n",
    "# If not, load it or define it based on the saved model/scaler\n",
    "# feature_cols = [...]\n",
    "\n",
    "try:\n",
    "    # Check if climate file path is defined\n",
    "    if 'CLIMATE_FILE' not in locals() or not CLIMATE_FILE:\n",
    "        raise ValueError(\"CLIMATE_FILE path not defined in Cell #2.\")\n",
    "    if not os.path.exists(CLIMATE_FILE):\n",
    "        raise FileNotFoundError(f\"Climate feature file not found: {CLIMATE_FILE}\")\n",
    "\n",
    "    # Load the parquet/CSV containing county-monthly features\n",
    "    logger.info(f\"Loading historical climate features from: {CLIMATE_FILE}\")\n",
    "    if CLIMATE_FILE.endswith(\".parquet\"):\n",
    "        df_climate_hist = pd.read_parquet(CLIMATE_FILE)\n",
    "    else:\n",
    "        df_climate_hist = pd.read_csv(CLIMATE_FILE, parse_dates=['time'])\n",
    "    logger.info(f\"Loaded climate data shape: {df_climate_hist.shape}\")\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if 'county_geoid' not in df_climate_hist.columns or 'time' not in df_climate_hist.columns:\n",
    "         raise ValueError(\"Climate data missing 'county_geoid' or 'time' columns.\")\n",
    "\n",
    "    # Ensure time is datetime and consistent format (e.g., month start)\n",
    "    df_climate_hist['time'] = pd.to_datetime(df_climate_hist['time']).dt.normalize() # Ensure start of day\n",
    "    # If time isn't already month start, uncomment below:\n",
    "    # df_climate_hist['time'] = df_climate_hist['time'] - pd.tseries.offsets.MonthBegin(1) # Ensure Month Start\n",
    "    df_climate_hist['year'] = df_climate_hist['time'].dt.year\n",
    "\n",
    "    # Filter climate data for the relevant historical period matching outage data / model training\n",
    "    # Determine required range (e.g., 2015 up to MAX_EVALUATION_YEAR)\n",
    "    hist_start_year_model = 2015 # Start year of model features\n",
    "    hist_end_year_model = MAX_EVALUATION_YEAR # Defined in Cell #2\n",
    "    logger.info(f\"Filtering climate features for relevant historical period: {hist_start_year_model}-{hist_end_year_model}\")\n",
    "    df_climate_hist = df_climate_hist[\n",
    "        (df_climate_hist['year'] >= hist_start_year_model) &\n",
    "        (df_climate_hist['year'] <= hist_end_year_model)\n",
    "    ].copy()\n",
    "\n",
    "    # Filter for target states (although file should already be filtered)\n",
    "    df_climate_hist['state_fips'] = df_climate_hist['county_geoid'].str[:2]\n",
    "    df_climate_hist = df_climate_hist[df_climate_hist['state_fips'].isin(target_state_fips)]\n",
    "\n",
    "    if df_climate_hist.empty:\n",
    "        raise ValueError(\"No historical climate data found for the specified period and states.\")\n",
    "\n",
    "    logger.info(f\"Historical climate features prepared. Shape: {df_climate_hist.shape}\")\n",
    "    display(df_climate_hist.head())\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Failed loading/preparing historical climate features: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW Cell 5.5: Recreate Features on Historical Climate Data ---\n",
    "\n",
    "logger.info(\"--- Recreating Features on Loaded Historical Climate Data ---\")\n",
    "\n",
    "try:\n",
    "    # Check inputs\n",
    "    if 'df_climate_hist' not in locals() or df_climate_hist.empty:\n",
    "        raise ValueError(\"df_climate_hist (from Cell 5) missing or empty.\")\n",
    "    if 'df_indices' not in locals() or df_indices.empty:\n",
    "        # Attempt to load indices if not present (assuming Cell 3.5 code exists)\n",
    "        logger.warning(\"df_indices not found, attempting to re-run index loading...\")\n",
    "        # You might need to copy the contents of Cell 3.5 here or ensure it runs first\n",
    "        # For now, assume it needs to be available\n",
    "        raise ValueError(\"Climate indices (df_indices) missing. Ensure index loading cell ran.\")\n",
    "    if 'TARGET_VARIABLES' not in locals(): raise NameError(\"TARGET_VARIABLES missing.\")\n",
    "    if 'LAG_MONTHS' not in locals(): raise NameError(\"LAG_MONTHS missing.\")\n",
    "\n",
    "    # --- Identify Variable Types ---\n",
    "    df_climate_hist['year'] = df_climate_hist['time'].dt.year # Ensure year/month exist\n",
    "    df_climate_hist['month'] = df_climate_hist['time'].dt.month\n",
    "    fixed_var_names = ['Altitude']\n",
    "    known_non_climate_cols = ['county_geoid', 'time', 'year', 'month', 'state_fips'] # Include state_fips if added\n",
    "    all_climate_cols = [col for col in df_climate_hist.columns if col not in known_non_climate_cols and not col.endswith('_clim') and not col.endswith('_anom')]\n",
    "    time_varying_climate_vars = [col for col in all_climate_cols if col not in fixed_var_names]\n",
    "    fixed_climate_vars = [col for col in all_climate_cols if col in fixed_var_names]\n",
    "    logger.info(f\"Identified Time-Varying Vars: {time_varying_climate_vars}\")\n",
    "    logger.info(f\"Identified Fixed Vars: {fixed_climate_vars}\")\n",
    "\n",
    "    # --- Calculate Climatology & Anomalies ---\n",
    "    logger.info(\"Calculating monthly climatology and anomalies...\")\n",
    "    anomaly_cols = []\n",
    "    climatology_cols = []\n",
    "    for var in time_varying_climate_vars + fixed_climate_vars:\n",
    "        clim_col = f'{var}_clim'; anom_col = f'{var}_anom'; climatology_cols.append(clim_col)\n",
    "        df_climate_hist[clim_col] = df_climate_hist.groupby(['county_geoid', 'month'])[var].transform('mean')\n",
    "        df_climate_hist[anom_col] = df_climate_hist[var] - df_climate_hist[clim_col]; anomaly_cols.append(anom_col)\n",
    "    logger.info(\"Anomaly calculation complete.\")\n",
    "\n",
    "    # --- Merge Climate Indices ---\n",
    "    logger.info(\"Merging climate indices...\")\n",
    "    # Ensure year columns have same type for merging\n",
    "    df_climate_hist['year'] = df_climate_hist['year'].astype(df_indices['year'].dtype)\n",
    "    df_hist_featured = pd.merge(df_climate_hist, df_indices, on='year', how='left') # Use df_climate_hist as base\n",
    "    logger.debug(f\"Shape after merging indices: {df_hist_featured.shape}\")\n",
    "    index_cols = [col for col in df_indices.columns if col != 'year']\n",
    "    index_nan_counts = df_hist_featured[index_cols].isnull().sum()\n",
    "    if index_nan_counts.sum() > 0: logger.warning(f\"NaNs found in merged indices:\\n{index_nan_counts[index_nan_counts > 0]}\")\n",
    "\n",
    "    # --- Impute Missing Values (especially for indices) ---\n",
    "    impute_cols = anomaly_cols + index_cols + fixed_climate_vars\n",
    "    impute_cols = [col for col in impute_cols if col in df_hist_featured.columns]\n",
    "    logger.info(f\"Imputing NaNs in features: {impute_cols} ...\")\n",
    "    df_hist_featured.sort_values(by=['county_geoid', 'time'], inplace=True)\n",
    "    anom_impute_cols = [c for c in anomaly_cols if c in df_hist_featured.columns]; index_impute_cols = [c for c in index_cols if c in df_hist_featured.columns]; fixed_impute_cols = [c for c in fixed_climate_vars if c in df_hist_featured.columns]\n",
    "    if anom_impute_cols: df_hist_featured[anom_impute_cols] = df_hist_featured.groupby('county_geoid')[anom_impute_cols].ffill().bfill()\n",
    "    if index_impute_cols: df_hist_featured[index_impute_cols] = df_hist_featured[index_impute_cols].ffill().bfill()\n",
    "    if fixed_impute_cols:\n",
    "        for fv in fixed_impute_cols:\n",
    "             if df_hist_featured[fv].isnull().any(): med = df_hist_featured[fv].median(); df_hist_featured[fv].fillna(med, inplace=True)\n",
    "    remaining_nans = df_hist_featured[impute_cols].isnull().sum()\n",
    "    if remaining_nans.sum() > 0: logger.warning(f\"NaNs remain after imputation:\\n{remaining_nans[remaining_nans > 0]}\"); logger.warning(\"Dropping rows.\"); df_hist_featured.dropna(subset=impute_cols, inplace=True)\n",
    "\n",
    "\n",
    "    # --- Create Lagged Features for ANOMALIES ---\n",
    "    logger.info(f\"Creating lagged features for months: {LAG_MONTHS}...\")\n",
    "    lag_cols = []\n",
    "    time_varying_anom_cols = [f'{var}_anom' for var in time_varying_climate_vars if f'{var}_anom' in df_hist_featured.columns]\n",
    "    if time_varying_anom_cols:\n",
    "        for lag in LAG_MONTHS:\n",
    "            for anom_var in time_varying_anom_cols: lag_col_name = f'{anom_var}_lag{lag}'; df_hist_featured = df_hist_featured.assign(**{lag_col_name: df_hist_featured.groupby('county_geoid')[anom_var].shift(lag)}); lag_cols.append(lag_col_name)\n",
    "\n",
    "    # --- Create Time Features ---\n",
    "    df_hist_featured['month_sin'] = np.sin(2 * np.pi * df_hist_featured['month']/12)\n",
    "    df_hist_featured['month_cos'] = np.cos(2 * np.pi * df_hist_featured['month']/12)\n",
    "\n",
    "    # --- Drop rows with NaNs introduced by lagging ---\n",
    "    logger.info(f\"Dropping rows with lag NaNs.\")\n",
    "    lag_cols_exist = [col for col in lag_cols if col in df_hist_featured.columns]\n",
    "    if lag_cols_exist: df_hist_featured.dropna(subset=lag_cols_exist, inplace=True)\n",
    "    logger.info(f\"Shape after feature engineering & lag drop: {df_hist_featured.shape}\")\n",
    "    if df_hist_featured.empty: raise ValueError(\"DataFrame empty after lag drop.\")\n",
    "\n",
    "    # --- Final Check for Feature Columns ---\n",
    "    # feature_cols list should be available from Cell #6 (loaded from scaler)\n",
    "    if 'feature_cols' not in locals() or not feature_cols:\n",
    "         raise NameError(\"feature_cols list not found. Ensure Cell #6 ran.\")\n",
    "    missing_features_final = [col for col in feature_cols if col not in df_hist_featured.columns]\n",
    "    if missing_features_final:\n",
    "        raise ValueError(f\"Features expected by model are STILL missing after engineering: {missing_features_final}\")\n",
    "\n",
    "    logger.info(\"Feature engineering complete for historical climate data.\")\n",
    "    display(df_hist_featured.head())\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Failed during historical feature engineering: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-14 13:25:47] [INFO] OutageAnalysisLogger - --- Loading Scaler and Trained Storm Prediction Model ---\n",
      "[2025-04-14 13:25:47] [INFO] OutageAnalysisLogger - Loading scaler from: ../output/models/scs_wind_target_states_scaler.joblib\n",
      "[2025-04-14 13:25:47] [INFO] OutageAnalysisLogger - Scaler loaded successfully.\n",
      "[2025-04-14 13:25:47] [INFO] OutageAnalysisLogger - Retrieved 35 feature names from scaler.\n",
      "[2025-04-14 13:25:47] [INFO] OutageAnalysisLogger - Loading model from: ../output/models/scs_wind_target_AnomInd_lgbm_calibrated.joblib\n",
      "[2025-04-14 13:25:47] [INFO] OutageAnalysisLogger - Storm prediction model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6: Load Scaler & Trained Storm Model ---\n",
    "\n",
    "logger.info(\"--- Loading Scaler and Trained Storm Prediction Model ---\")\n",
    "\n",
    "# Paths defined in Cell #2\n",
    "# SCALER_PATH = os.path.join(MODEL_OUTPUT_DIR, f\"{TARGET_EVENT_TYPE}_{TARGET_STATE_ABBR}_AnomInd_scaler.joblib\")\n",
    "# MODEL_PATH = os.path.join(MODEL_OUTPUT_DIR, f\"{TARGET_EVENT_TYPE}_{TARGET_STATE_ABBR}_AnomInd_lgbm_calibrated.joblib\")\n",
    "\n",
    "scaler = None\n",
    "model = None\n",
    "feature_cols = [] # Initialize\n",
    "\n",
    "try:\n",
    "    # Load Scaler\n",
    "    logger.info(f\"Loading scaler from: {SCALER_PATH}\")\n",
    "    if not os.path.exists(SCALER_PATH): raise FileNotFoundError(f\"Scaler file not found: {SCALER_PATH}\")\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    logger.info(\"Scaler loaded successfully.\")\n",
    "    # Get feature names scaler was trained on\n",
    "    if hasattr(scaler, 'feature_names_in_'):\n",
    "        feature_cols = list(scaler.feature_names_in_)\n",
    "        logger.info(f\"Retrieved {len(feature_cols)} feature names from scaler.\")\n",
    "    else:\n",
    "         # Attempt to infer from climate data if needed (less reliable)\n",
    "         if 'df_climate_hist' in locals() and not df_climate_hist.empty:\n",
    "              cols_to_exclude = ['county_geoid', 'time', 'year', 'month', 'state_fips'] + [c for c in df_climate_hist if c.endswith('_clim')] # Exclude keys, base vars, clim vars\n",
    "              feature_cols = [c for c in df_climate_hist.columns if c not in cols_to_exclude]\n",
    "              logger.warning(f\"Scaler missing feature names. Inferred {len(feature_cols)} features from climate data columns. Ensure this is correct!\")\n",
    "         else:\n",
    "              raise ValueError(\"Cannot determine feature columns: Scaler lacks names and climate data not loaded.\")\n",
    "\n",
    "    # Load Model\n",
    "    logger.info(f\"Loading model from: {MODEL_PATH}\")\n",
    "    if not os.path.exists(MODEL_PATH): raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n",
    "    model = joblib.load(MODEL_PATH) # Load the calibrated LGBM model\n",
    "    logger.info(\"Storm prediction model loaded successfully.\")\n",
    "    logger.debug(f\"Model type: {type(model)}\")\n",
    "\n",
    "except FileNotFoundError as fnf: logger.exception(f\"{fnf}\"); raise fnf\n",
    "except ValueError as ve: logger.exception(f\"{ve}\"); raise ve\n",
    "except Exception as e: logger.exception(f\"Failed loading model/scaler: {e}\"); raise e\n",
    "\n",
    "# Display first few feature names\n",
    "if feature_cols: logger.debug(f\"Model expects features like: {feature_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-14 13:38:23] [INFO] OutageAnalysisLogger - --- Generating Historical Storm Likelihood Predictions ---\n",
      "[2025-04-14 13:38:23] [ERROR] OutageAnalysisLogger - Failed generating historical predictions: Engineered historical features (df_hist_featured from Cell 5.5) missing or empty.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\60864\\AppData\\Local\\Temp\\ipykernel_36664\\2536494032.py\", line 10, in <module>\n",
      "    raise ValueError(\"Engineered historical features (df_hist_featured from Cell 5.5) missing or empty.\")\n",
      "ValueError: Engineered historical features (df_hist_featured from Cell 5.5) missing or empty.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Engineered historical features (df_hist_featured from Cell 5.5) missing or empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 67\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     66\u001b[39m     logger.exception(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed generating historical predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# --- Check inputs ---\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# *** Use the DataFrame with engineered features from Cell 5.5 ***\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdf_hist_featured\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mor\u001b[39;00m df_hist_featured.empty:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngineered historical features (df_hist_featured from Cell 5.5) missing or empty.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mor\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mScaler not loaded (Cell 6).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m() \u001b[38;5;129;01mor\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mModel not loaded (Cell 6).\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Engineered historical features (df_hist_featured from Cell 5.5) missing or empty."
     ]
    }
   ],
   "source": [
    "# --- Cell 7: Generate Historical Storm Predictions (using df_hist_featured) ---\n",
    "\n",
    "logger.info(\"--- Generating Historical Storm Likelihood Predictions ---\")\n",
    "df_hist_pred = pd.DataFrame() # Initialize\n",
    "\n",
    "try:\n",
    "    # --- Check inputs ---\n",
    "    # *** Use the DataFrame with engineered features from Cell 5.5 ***\n",
    "    if 'df_hist_featured' not in locals() or df_hist_featured.empty:\n",
    "        raise ValueError(\"Engineered historical features (df_hist_featured from Cell 5.5) missing or empty.\")\n",
    "    if 'scaler' not in locals() or scaler is None: raise NameError(\"Scaler not loaded (Cell 6).\")\n",
    "    if 'model' not in locals() or model is None: raise NameError(\"Model not loaded (Cell 6).\")\n",
    "    if 'feature_cols' not in locals() or not feature_cols: raise ValueError(\"feature_cols list missing (Cell 6).\")\n",
    "    # --- End Checks ---\n",
    "\n",
    "\n",
    "    # --- Prepare historical features (Select feature columns from df_hist_featured) ---\n",
    "    logger.info(\"Selecting final feature columns from engineered historical data...\")\n",
    "    # Ensure feature columns exist (should be guaranteed by check in Cell 5.5)\n",
    "    missing_features = [col for col in feature_cols if col not in df_hist_featured.columns]\n",
    "    if missing_features:\n",
    "        # This shouldn't happen if Cell 5.5 ran correctly, but double-check\n",
    "        raise ValueError(f\"Engineered historical data missing required features: {missing_features}\")\n",
    "\n",
    "    # Use the order defined by the scaler\n",
    "    feature_names_ordered = scaler.feature_names_in_ if hasattr(scaler, 'feature_names_in_') else feature_cols\n",
    "    X_hist = df_hist_featured[feature_names_ordered].copy() # Select from df_hist_featured\n",
    "\n",
    "\n",
    "    # --- Scale features ---\n",
    "    logger.info(f\"Scaling historical features (shape: {X_hist.shape})...\")\n",
    "    # Check for NaNs BEFORE scaling (should have been handled in Cell 5.5, but check again)\n",
    "    nan_check = X_hist.isnull().sum()\n",
    "    if nan_check.sum() > 0:\n",
    "        logger.error(f\"Unexpected NaNs found in features BEFORE scaling:\\n{nan_check[nan_check > 0]}\")\n",
    "        raise ValueError(\"NaNs found in features before scaling, imputation in Cell 5.5 failed.\")\n",
    "\n",
    "    X_hist_scaled = scaler.transform(X_hist)\n",
    "    logger.info(\"Historical features scaled.\")\n",
    "\n",
    "    # --- Predict probabilities ---\n",
    "    logger.info(\"Predicting historical probabilities...\")\n",
    "    with warnings.catch_warnings(): warnings.filterwarnings(\"ignore\"); hist_pred_proba = model.predict_proba(X_hist_scaled)[:, 1]\n",
    "\n",
    "    # --- Combine predictions with identifiers ---\n",
    "    # Get identifiers from df_hist_featured (ensure time and state_fips are there)\n",
    "    identifier_cols = ['county_geoid', 'time', 'state_fips']\n",
    "    if not all(col in df_hist_featured.columns for col in identifier_cols):\n",
    "        raise ValueError(f\"df_hist_featured missing identifier columns: Need {identifier_cols}\")\n",
    "\n",
    "    df_hist_pred = df_hist_featured[identifier_cols].copy()\n",
    "    # Important: Ensure index alignment if X_hist was created differently\n",
    "    if not df_hist_pred.index.equals(X_hist.index):\n",
    "         logger.warning(\"Index mismatch between identifiers and features. Resetting index.\")\n",
    "         df_hist_pred.reset_index(drop=True, inplace=True)\n",
    "         # Need to align probs based on the original index before it was potentially dropped\n",
    "         # Safer: Assign directly if confident lengths match after all processing\n",
    "         if len(df_hist_pred) != len(hist_pred_proba):\n",
    "             raise ValueError(\"Length mismatch after processing, cannot assign probabilities.\")\n",
    "\n",
    "    df_hist_pred['predicted_prob'] = hist_pred_proba\n",
    "    logger.info(\"Historical probabilities generated.\")\n",
    "    display(df_hist_pred.head())\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(f\"Failed generating historical predictions: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 8: Aggregate Historical Predictions to State-Month ---\n",
    "\n",
    "logger.info(\"--- Aggregating Historical Predictions to State-Month ---\")\n",
    "df_hist_state_monthly_expected = pd.DataFrame() # Initialize\n",
    "\n",
    "if 'df_hist_pred' not in locals() or df_hist_pred.empty:\n",
    "    logger.error(\"Historical predictions (df_hist_pred) missing. Cannot aggregate.\")\n",
    "else:\n",
    "    try:\n",
    "        logger.info(\"Summing county probabilities to get state-level expected hits...\")\n",
    "        # Group by state and month-start time, sum probabilities\n",
    "        df_hist_state_monthly_expected = df_hist_pred.groupby(\n",
    "            ['state_fips', pd.Grouper(key='time', freq='MS')]\n",
    "        )['predicted_prob'].sum().reset_index()\n",
    "        # Rename for clarity\n",
    "        df_hist_state_monthly_expected.rename(columns={'predicted_prob': 'pred_expected_hits_hist'}, inplace=True)\n",
    "\n",
    "        logger.info(f\"Aggregation complete. Shape: {df_hist_state_monthly_expected.shape}\")\n",
    "        print(\"\\n--- Sample State-Month Historical Expected Hits ---\")\n",
    "        display(df_hist_state_monthly_expected.head())\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Failed aggregating historical predictions: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 9: Merge Outage Metrics & Storm Predictions ---\n",
    "\n",
    "logger.info(\"--- Merging Monthly Outage Data with Monthly Predicted Hits ---\")\n",
    "df_merged_outage_pred = pd.DataFrame() # Initialize\n",
    "\n",
    "try:\n",
    "    # Check inputs\n",
    "    if 'df_outages_monthly_agg' not in locals() or df_outages_monthly_agg.empty:\n",
    "        raise ValueError(\"Aggregated monthly outage data (df_outages_monthly_agg) not available.\")\n",
    "    if 'df_hist_state_monthly_expected' not in locals() or df_hist_state_monthly_expected.empty:\n",
    "        raise ValueError(\"Aggregated historical predicted hits (df_hist_state_monthly_expected) not available.\")\n",
    "\n",
    "    # Ensure time columns are datetime\n",
    "    df_outages_monthly_agg['time'] = pd.to_datetime(df_outages_monthly_agg['time'])\n",
    "    df_hist_state_monthly_expected['time'] = pd.to_datetime(df_hist_state_monthly_expected['time'])\n",
    "\n",
    "    logger.info(\"Performing merge on 'state_fips' and 'time'...\")\n",
    "    logger.debug(f\"Outage data shape: {df_outages_monthly_agg.shape}, Time range: {df_outages_monthly_agg['time'].min()} - {df_outages_monthly_agg['time'].max()}\")\n",
    "    logger.debug(f\"Prediction data shape: {df_hist_state_monthly_expected.shape}, Time range: {df_hist_state_monthly_expected['time'].min()} - {df_hist_state_monthly_expected['time'].max()}\")\n",
    "\n",
    "    # Use inner merge to keep only state-months present in both datasets\n",
    "    df_merged_outage_pred = pd.merge(\n",
    "        df_outages_monthly_agg,\n",
    "        df_hist_state_monthly_expected,\n",
    "        on=['state_fips', 'time'],\n",
    "        how='inner' # Use inner join to only analyze periods where both exist\n",
    "    )\n",
    "\n",
    "    if df_merged_outage_pred.empty:\n",
    "        logger.warning(\"Merge resulted in empty DataFrame. Check time ranges and state FIPS codes in both datasets.\")\n",
    "    else:\n",
    "        logger.info(f\"Merge successful. Final shape for analysis: {df_merged_outage_pred.shape}\")\n",
    "        logger.info(f\"Time range of merged data: {df_merged_outage_pred['time'].min()} - {df_merged_outage_pred['time'].max()}\")\n",
    "        print(\"\\n--- Sample Merged Outage and Prediction Data ---\")\n",
    "        display(df_merged_outage_pred.head())\n",
    "\n",
    "        # Check for NaNs in key columns after merge\n",
    "        key_cols_check = TARGET_OUTAGE_METRICS + ['peak_pct_out', 'pred_expected_hits_hist']\n",
    "        nan_check_merged = df_merged_outage_pred[[col for col in key_cols_check if col in df_merged_outage_pred.columns]].isnull().sum()\n",
    "        if nan_check_merged.sum() > 0:\n",
    "             logger.warning(f\"NaNs present in merged data columns:\\n{nan_check_merged[nan_check_merged > 0]}\")\n",
    "             # Optional: Drop rows with NaNs in target metric or predictor for modeling\n",
    "             # target_metric = 'peak_pct_out' # Choose metric\n",
    "             # df_merged_outage_pred.dropna(subset=[target_metric, 'pred_expected_hits_hist'], inplace=True)\n",
    "             # logger.info(f\"Shape after dropping NaNs for modeling: {df_merged_outage_pred.shape}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "     logger.exception(f\"Failed merging outage and prediction data: {e}\")\n",
    "     raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 10: Analyze Relationship & Build Linking Model ---\n",
    "import statsmodels.formula.api as smf\n",
    "import seaborn as sns # For correlation heatmap\n",
    "\n",
    "logger.info(\"--- Analyzing Relationship between Predicted Storms and Outages ---\")\n",
    "\n",
    "# Choose the target outage metric to analyze/model\n",
    "target_metric = 'peak_pct_out' # Example: Predict peak percentage of customers out\n",
    "# Or: target_metric = 'CustomerHoursOutTotal'\n",
    "# Or: target_metric = 'MaxCustomersOutTotal'\n",
    "\n",
    "# Ensure chosen metric and predictor exist and have no NaNs for correlation/modeling\n",
    "analysis_cols = [target_metric, 'pred_expected_hits_hist']\n",
    "if not all(col in df_merged_outage_pred.columns for col in analysis_cols):\n",
    "     logger.error(f\"Missing required columns for analysis: Need {analysis_cols}\")\n",
    "     # Stop or handle error\n",
    "else:\n",
    "    df_analysis = df_merged_outage_pred[analysis_cols].dropna()\n",
    "    logger.info(f\"Using {len(df_analysis)} complete records for analysis.\")\n",
    "\n",
    "    if len(df_analysis) > 1: # Need > 1 record for correlation/regression\n",
    "        # --- Correlation Analysis ---\n",
    "        logger.info(\"Calculating correlation...\")\n",
    "        correlation = df_analysis[target_metric].corr(df_analysis['pred_expected_hits_hist'])\n",
    "        logger.info(f\"Correlation between {target_metric} and Predicted Expected Hits: {correlation:.4f}\")\n",
    "        print(f\"\\nCorrelation ({target_metric} vs Pred. Hits): {correlation:.4f}\")\n",
    "\n",
    "        # Scatter Plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(data=df_analysis, x='pred_expected_hits_hist', y=target_metric, alpha=0.5)\n",
    "        plt.title(f'{target_metric} vs. Predicted Expected SCS Hits per State-Month')\n",
    "        plt.xlabel(\"Predicted Expected SCS Hits (Sum of County Probs)\")\n",
    "        plt.ylabel(f\"State-Month {target_metric}\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # --- Simple Linking Model Example (OLS Regression) ---\n",
    "        # Predicting outage metric based on predicted storm likelihood\n",
    "        # Formula requires valid names (no spaces etc.)\n",
    "        df_analysis = df_analysis.rename(columns={\n",
    "            target_metric: 'Outage_Metric',\n",
    "            'pred_expected_hits_hist': 'Pred_Expected_Hits'\n",
    "        })\n",
    "        formula = \"Outage_Metric ~ Pred_Expected_Hits\"\n",
    "        try:\n",
    "            logger.info(f\"Fitting OLS model: {formula}\")\n",
    "            ols_model = smf.ols(formula, data=df_analysis).fit()\n",
    "            logger.info(\"OLS linking model fitted.\")\n",
    "            print(\"\\n--- Simple Linking Model Summary (OLS) ---\")\n",
    "            print(ols_model.summary())\n",
    "\n",
    "            # Could save this model: ols_model.save(\"path/to/linking_model.pickle\")\n",
    "\n",
    "            # Add predictions from linking model for comparison\n",
    "            df_analysis['Linked_Pred_Outage'] = ols_model.predict(df_analysis[['Pred_Expected_Hits']])\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(df_analysis['Outage_Metric'], df_analysis['Linked_Pred_Outage'], alpha=0.3, s=10)\n",
    "            plot_min = min(0, df_analysis['Outage_Metric'].min(), df_analysis['Linked_Pred_Outage'].min())\n",
    "            plot_max = max(df_analysis['Outage_Metric'].max(), df_analysis['Linked_Pred_Outage'].max()) * 1.05\n",
    "            plt.plot([plot_min, plot_max], [plot_min, plot_max], 'r--', label='Ideal Fit')\n",
    "            plt.xlabel(f\"Actual {target_metric}\")\n",
    "            plt.ylabel(f\"Predicted {target_metric} (from Storm Hits)\")\n",
    "            plt.title('Linking Model: Actual vs. Predicted Outage Metric')\n",
    "            plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "        except Exception as model_e:\n",
    "            logger.exception(f\"Failed to fit/evaluate linking model: {model_e}\")\n",
    "\n",
    "    else:\n",
    "        logger.warning(\"Not enough data points for correlation/regression analysis after dropping NaNs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 11: Future Outage Projections (Optional) ---\n",
    "\n",
    "logger.info(\"--- Generating Future Outage Projections using Linking Model ---\")\n",
    "\n",
    "# Check if linking model was created\n",
    "if 'ols_model' not in locals() or ols_model is None:\n",
    "     logger.warning(\"Linking model (ols_model) not found or failed to train. Skipping future outage projection.\")\n",
    "else:\n",
    "     try:\n",
    "          # --- Load Future Storm Projections ---\n",
    "          # Path defined in Cell #2 - points to ANNUAL county projections\n",
    "          # Need MONTHLY state projections - recalculate here from X_future\n",
    "          logger.info(\"Generating MONTHLY state-level future storm predictions...\")\n",
    "\n",
    "          # Check required inputs\n",
    "          if 'X_future' not in locals() or X_future.empty: raise ValueError(\"X_future missing.\")\n",
    "          if 'future_identifiers' not in locals() or future_identifiers.empty: raise ValueError(\"future_identifiers missing.\")\n",
    "          if 'scaler' not in locals() or scaler is None: raise NameError(\"Scaler missing.\")\n",
    "          if 'model' not in locals() or model is None: raise NameError(\"Storm model missing.\")\n",
    "          if not feature_cols: raise ValueError(\"feature_cols missing.\")\n",
    "\n",
    "          # Prepare/Scale future features\n",
    "          logger.debug(\"Preparing/Scaling future climate features...\")\n",
    "          X_future_prep = X_future[feature_cols].copy()\n",
    "          X_future_scaled = scaler.transform(X_future_prep)\n",
    "\n",
    "          # Predict future probabilities\n",
    "          logger.debug(\"Predicting future probabilities...\")\n",
    "          with warnings.catch_warnings(): warnings.filterwarnings(\"ignore\", category=UserWarning); future_probs = model.predict_proba(X_future_scaled)[:, 1]\n",
    "\n",
    "          # Combine with identifiers\n",
    "          df_future_pred_monthly = future_identifiers.copy(); df_future_pred_monthly.reset_index(drop=True, inplace=True)\n",
    "          df_future_pred_monthly['predicted_prob'] = future_probs\n",
    "          df_future_pred_monthly['time'] = pd.to_datetime(df_future_pred_monthly['time'])\n",
    "          # Add state fips\n",
    "          df_future_pred_monthly['state_fips'] = df_future_pred_monthly['county_geoid'].str[:2]\n",
    "          df_future_pred_monthly = df_future_pred_monthly[df_future_pred_monthly['state_fips'].isin(target_state_fips)] # Ensure only target states\n",
    "\n",
    "          # Aggregate future probabilities to state-month\n",
    "          logger.debug(\"Aggregating future probabilities to state-month...\")\n",
    "          df_future_state_monthly_expected = df_future_pred_monthly.groupby(\n",
    "               ['state_fips', pd.Grouper(key='time', freq='MS')]\n",
    "          )['predicted_prob'].sum().reset_index()\n",
    "          df_future_state_monthly_expected.rename(columns={'predicted_prob': 'Pred_Expected_Hits'}, inplace=True) # Use same name as linking model input\n",
    "\n",
    "          if df_future_state_monthly_expected.empty:\n",
    "               raise ValueError(\"Future state-level storm predictions are empty.\")\n",
    "\n",
    "          # --- Apply Linking Model ---\n",
    "          logger.info(\"Applying linking model to future storm predictions...\")\n",
    "          # Use the linking model (ols_model) trained in Cell #10\n",
    "          # Ensure input columns match (needs 'Pred_Expected_Hits')\n",
    "          future_outage_predictions = ols_model.predict(df_future_state_monthly_expected[['Pred_Expected_Hits']])\n",
    "          # Ensure non-negative if predicting metrics like hours/counts\n",
    "          # if target_metric in ['CustomerHoursOutTotal', 'MaxCustomersOutTotal']:\n",
    "          #      future_outage_predictions = np.maximum(0, future_outage_predictions)\n",
    "\n",
    "          df_future_outage_proj = df_future_state_monthly_expected[['state_fips', 'time']].copy()\n",
    "          df_future_outage_proj[f'Projected_{target_metric}'] = future_outage_predictions\n",
    "\n",
    "          logger.info(\"Future outage projections generated.\")\n",
    "          print(f\"\\n--- Sample Future Projected Outages ({target_metric}) ---\")\n",
    "          display(df_future_outage_proj.head())\n",
    "\n",
    "          # --- Save Future Outage Projections ---\n",
    "          OUTAGE_PROJ_PATH = os.path.join(ANALYSIS_OUTPUT_DIR, f\"projected_{target_metric}_by_state_month.csv\")\n",
    "          try:\n",
    "               df_future_outage_proj.to_csv(OUTAGE_PROJ_PATH, index=False)\n",
    "               logger.info(f\"Saved future outage projections to {OUTAGE_PROJ_PATH}\")\n",
    "          except Exception as save_e:\n",
    "               logger.exception(f\"Failed to save outage projections: {save_e}\")\n",
    "\n",
    "          # --- Plot Future Outage Projections (Example: Time Series for a State) ---\n",
    "          state_to_plot = target_state_fips[0] # Plot first state\n",
    "          plot_df_future = df_future_outage_proj[df_future_outage_proj['state_fips'] == state_to_plot]\n",
    "          if not plot_df_future.empty:\n",
    "               plt.figure(figsize=(15, 6))\n",
    "               plt.plot(plot_df_future['time'], plot_df_future[f'Projected_{target_metric}'], marker='.', linestyle='-')\n",
    "               plt.title(f\"Projected Future {target_metric} for State {state_to_plot}\")\n",
    "               plt.xlabel(\"Time\"); plt.ylabel(f\"Projected {target_metric}\")\n",
    "               plt.grid(True); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "     except FileNotFoundError as fnf: logger.exception(f\"{fnf}\")\n",
    "     except ValueError as ve: logger.exception(f\"ValueError: {ve}\")\n",
    "     except NameError as ne: logger.exception(f\"Missing variable: {ne}\")\n",
    "     except Exception as e: logger.exception(f\"Failed future outage projection: {e}\")\n",
    "\n",
    "logger.info(\"--- Outage Analysis Notebook Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
